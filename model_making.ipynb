{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:05:06.132340Z","iopub.status.busy":"2023-04-22T04:05:06.131553Z","iopub.status.idle":"2023-04-22T04:05:06.137236Z","shell.execute_reply":"2023-04-22T04:05:06.135917Z","shell.execute_reply.started":"2023-04-22T04:05:06.132300Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:05:07.381748Z","iopub.status.busy":"2023-04-22T04:05:07.381376Z","iopub.status.idle":"2023-04-22T04:05:14.389295Z","shell.execute_reply":"2023-04-22T04:05:14.388176Z","shell.execute_reply.started":"2023-04-22T04:05:07.381715Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', encoding='UTF-8', encoding_errors='ignore')\n","data.columns = ['target','id','date','flag','user','text']\n","data['target'] = data['target'].apply(lambda x: 0 if x==0 else 1)\n","# groups = data.groupby('target')\n","# n = 250000\n","\n","# # Define a function to sample n rows from each group\n","# def sample_group(group):\n","#     return group.sample(n=n, replace=True)\n","# data = groups.apply(sample_group).reset_index(drop=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:05:18.506419Z","iopub.status.busy":"2023-04-22T04:05:18.505710Z","iopub.status.idle":"2023-04-22T04:05:18.526436Z","shell.execute_reply":"2023-04-22T04:05:18.525472Z","shell.execute_reply.started":"2023-04-22T04:05:18.506381Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811372</td>\n","      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>joy_wolf</td>\n","      <td>@Kwesidei not the whole crew</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   target          id                          date      flag           user  \\\n","0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n","1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n","2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n","3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n","4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n","\n","                                                text  \n","0  is upset that he can't update his Facebook by ...  \n","1  @Kenichan I dived many times for the ball. Man...  \n","2    my whole body feels itchy and like its on fire   \n","3  @nationwideclass no, it's not behaving at all....  \n","4                      @Kwesidei not the whole crew   "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:05:18.851418Z","iopub.status.busy":"2023-04-22T04:05:18.851036Z","iopub.status.idle":"2023-04-22T04:05:18.893430Z","shell.execute_reply":"2023-04-22T04:05:18.892298Z","shell.execute_reply.started":"2023-04-22T04:05:18.851383Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@Kwesidei not the whole crew</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   target                                               text\n","0       0  is upset that he can't update his Facebook by ...\n","1       0  @Kenichan I dived many times for the ball. Man...\n","2       0    my whole body feels itchy and like its on fire \n","3       0  @nationwideclass no, it's not behaving at all....\n","4       0                      @Kwesidei not the whole crew "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data.drop(columns=['id','date','flag','user'], inplace=True)\n","data.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:05:19.548153Z","iopub.status.busy":"2023-04-22T04:05:19.546981Z","iopub.status.idle":"2023-04-22T04:05:36.788035Z","shell.execute_reply":"2023-04-22T04:05:36.786819Z","shell.execute_reply.started":"2023-04-22T04:05:19.548101Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1599999/1599999 [00:17<00:00, 93119.16it/s] \n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","      <td>is upset that he cant update his facebook by t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","      <td>i dived many times for the ball managed to sav...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","      <td>no its not behaving at all im mad why am i her...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@Kwesidei not the whole crew</td>\n","      <td>not the whole crew</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599994</th>\n","      <td>1</td>\n","      <td>Just woke up. Having no school is the best fee...</td>\n","      <td>just woke up having no school is the best feel...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>1</td>\n","      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n","      <td>thewdbcom very cool to hear old walt interview...</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>1</td>\n","      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n","      <td>are you ready for your mojo makeover ask me fo...</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>1</td>\n","      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n","      <td>happy th birthday to my boo of alll time tupac...</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>1</td>\n","      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n","      <td>happy charitytuesday</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1599999 rows × 3 columns</p>\n","</div>"],"text/plain":["         target                                               text  \\\n","0             0  is upset that he can't update his Facebook by ...   \n","1             0  @Kenichan I dived many times for the ball. Man...   \n","2             0    my whole body feels itchy and like its on fire    \n","3             0  @nationwideclass no, it's not behaving at all....   \n","4             0                      @Kwesidei not the whole crew    \n","...         ...                                                ...   \n","1599994       1  Just woke up. Having no school is the best fee...   \n","1599995       1  TheWDB.com - Very cool to hear old Walt interv...   \n","1599996       1  Are you ready for your MoJo Makeover? Ask me f...   \n","1599997       1  Happy 38th Birthday to my boo of alll time!!! ...   \n","1599998       1  happy #charitytuesday @theNSPCC @SparksCharity...   \n","\n","                                                clean_text  \n","0        is upset that he cant update his facebook by t...  \n","1        i dived many times for the ball managed to sav...  \n","2           my whole body feels itchy and like its on fire  \n","3        no its not behaving at all im mad why am i her...  \n","4                                       not the whole crew  \n","...                                                    ...  \n","1599994  just woke up having no school is the best feel...  \n","1599995  thewdbcom very cool to hear old walt interview...  \n","1599996  are you ready for your mojo makeover ask me fo...  \n","1599997  happy th birthday to my boo of alll time tupac...  \n","1599998                               happy charitytuesday  \n","\n","[1599999 rows x 3 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","\n","def clean_text(text):\n","    # remove words starting with @ symbol\n","    text = re.sub(r'@\\w+\\s?', '', text)\n","    \n","    # remove non-alphabetic characters and convert to lowercase\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n","    \n","    # remove unnecessary whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    \n","    return text\n","\n","def clean_data(data):\n","    # initialize progress bar\n","    tqdm.pandas()\n","    \n","    # apply clean_text function to each sentence in text column\n","    data['clean_text'] = data['text'].progress_apply(clean_text)\n","    \n","    return data\n","\n","clean_data(data)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:05:36.790812Z","iopub.status.busy":"2023-04-22T04:05:36.790164Z","iopub.status.idle":"2023-04-22T04:08:36.217567Z","shell.execute_reply":"2023-04-22T04:08:36.216179Z","shell.execute_reply.started":"2023-04-22T04:05:36.790773Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e567a68c5fc34607a42fbe34af818680","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed11a2bcaa54474f8f30034ca0e12549","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0405448e045440882e07c99d4406ab8","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e5cdddb565a4105a7c92ad3f9376789","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1599999/1599999 [02:56<00:00, 9043.76it/s] \n"]}],"source":["from transformers import AutoTokenizer\n","\n","def tokenize_data(data, model_name):\n","    # load pre-trained tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    tqdm.pandas()\n","    # tokenize each sentence in clean_text column\n","    data['tokens'] = data['clean_text'].progress_apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n","    vocab_dict = tokenizer.get_vocab()\n","    return data, vocab_dict, tokenizer\n","model_name = 'bert-base-uncased'\n","tokenized_data, vocab_dict, tokenizer = tokenize_data(data, model_name)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:08:38.125626Z","iopub.status.busy":"2023-04-22T04:08:38.124871Z","iopub.status.idle":"2023-04-22T04:08:38.409209Z","shell.execute_reply":"2023-04-22T04:08:38.408079Z","shell.execute_reply.started":"2023-04-22T04:08:38.125590Z"},"trusted":true},"outputs":[{"data":{"text/plain":["72"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["max_ = 0\n","for i in data['tokens'].values:\n","    if len(i) > max_:\n","        max_ = len(i)\n","max_"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:08:40.097529Z","iopub.status.busy":"2023-04-22T04:08:40.096791Z","iopub.status.idle":"2023-04-22T04:08:44.158158Z","shell.execute_reply":"2023-04-22T04:08:44.157009Z","shell.execute_reply.started":"2023-04-22T04:08:40.097478Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def pad_tokens(data, max_length):\n","    \"\"\"\n","    Pads the tokenized sentences in the 'tokens' column of the input DataFrame to a fixed length.\n","    \n","    Args:\n","        data (pandas.DataFrame): The input DataFrame with the 'tokens' column containing the tokenized sentences.\n","        max_length (int): The maximum length of the padded sentences.\n","    \n","    Returns:\n","        np.ndarray: A 2D NumPy array of shape (num_sentences, max_length) containing the padded token IDs.\n","    \"\"\"\n","    # Get the tokenized sentences as a list\n","    tokenized_sentences = data['tokens'].tolist()\n","    \n","    # Initialize an empty array to hold the padded token IDs\n","    padded_tokens = np.zeros((len(tokenized_sentences), max_length))\n","    \n","    # Pad each sentence with zeros to the desired length and store in the padded_tokens array\n","    for i, tokens in enumerate(tokenized_sentences):\n","        padded_tokens[i, :min(len(tokens), max_length)] = tokens[:max_length]\n","    \n","    return padded_tokens\n","padded_tokens = pad_tokens(data, max_)\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:08:46.335833Z","iopub.status.busy":"2023-04-22T04:08:46.335461Z","iopub.status.idle":"2023-04-22T04:08:46.349865Z","shell.execute_reply":"2023-04-22T04:08:46.348807Z","shell.execute_reply.started":"2023-04-22T04:08:46.335799Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, SpatialDropout1D, MultiHeadAttention, LayerNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def build_transformer_model(num_heads, feed_forward_dim, num_layers, max_seq_length, vocab_size, embedding_dim):\n","    # Define the input layer\n","    inputs = Input(shape=(max_seq_length,), dtype=tf.int32)\n","\n","    # Define the embedding layer\n","    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length)(inputs)\n","    embedding_layer = SpatialDropout1D(0.2)(embedding_layer)\n","\n","    # Define the transformer layers\n","    transformer_layers = []\n","    for i in range(num_layers):\n","        transformer_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim, dropout=0.2)(embedding_layer, embedding_layer)\n","        transformer_layer = Dropout(0.2)(transformer_layer)\n","        transformer_layer = LayerNormalization(epsilon=1e-6)(transformer_layer)\n","        transformer_layer = Dense(feed_forward_dim, activation='relu')(transformer_layer)\n","        transformer_layer = Dense(embedding_dim)(transformer_layer)\n","        transformer_layer = Dropout(0.2)(transformer_layer)\n","        transformer_layer = LayerNormalization(epsilon=1e-6)(transformer_layer)\n","        transformer_layers.append(transformer_layer)\n","    transformer_output = tf.keras.layers.concatenate(transformer_layers)\n","\n","    # Define the output layer\n","    output_layer = Dense(1, activation='sigmoid')(transformer_output)\n","\n","    # Create the model\n","    model = Model(inputs=inputs, outputs=output_layer)\n","\n","    # Compile the model\n","    optimizer = Adam(lr=0.00001)\n","    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:08:57.591763Z","iopub.status.busy":"2023-04-22T04:08:57.590811Z","iopub.status.idle":"2023-04-22T04:09:01.888358Z","shell.execute_reply":"2023-04-22T04:09:01.884726Z","shell.execute_reply.started":"2023-04-22T04:08:57.591725Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","X_train,X_val,y_train, y_val = train_test_split(padded_tokens,data['target'], test_size=0.2, random_state=42)\n","\n","num_heads = 8\n","feed_forward_dim = 512\n","num_layers = 4\n","max_seq_length = max_\n","vocab_size = len(vocab_dict)\n","embedding_dim = 64\n","\n","model = build_transformer_model(num_heads, feed_forward_dim, num_layers, max_seq_length, vocab_size, embedding_dim)\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=256, shuffle=True)\n","model.save('whole_senti_tweet_5_epoch.h5')\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:16:24.285339Z","iopub.status.busy":"2023-04-22T04:16:24.284801Z","iopub.status.idle":"2023-04-22T04:16:24.295550Z","shell.execute_reply":"2023-04-22T04:16:24.293980Z","shell.execute_reply.started":"2023-04-22T04:16:24.285286Z"},"trusted":true},"outputs":[],"source":["def predict_sentiment(model, tokenizer, sentence):\n","    # Clean and preprocess the input sentence\n","    clean_sentence = clean_text(sentence)\n","    tokenized_sentence = tokenizer.encode(clean_sentence, add_special_tokens=True)\n","    padded_sentence = pad_sequences([tokenized_sentence], maxlen=max_, dtype=\"long\", \n","                                     value=0, truncating=\"post\", padding=\"post\")\n","    # Predict the sentiment label\n","    predicted_label = model.predict(padded_sentence)\n","    print()\n","\n","    sentiment_dict = {0: 'Negative', 1: 'Positive'}\n","    predicted_sentiment = sentiment_dict[1 if predicted_label[0][0][0] > 0.5 else 0]\n","    \n","    return predicted_sentiment"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T04:12:37.866947Z","iopub.status.busy":"2023-04-22T04:12:37.866245Z","iopub.status.idle":"2023-04-22T04:12:37.941901Z","shell.execute_reply":"2023-04-22T04:12:37.940913Z","shell.execute_reply.started":"2023-04-22T04:12:37.866910Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 23ms/step\n","\n"]},{"data":{"text/plain":["'Negative'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["predict_sentiment(model,tokenizer,\"I am not fine\")"]}],"metadata":{"kernelspec":{"display_name":"reddit","language":"python","name":"reddit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
